{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st0RVQQxhiMU"
      },
      "source": [
        "Installing sacrebleu library for calculating the bleu of the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41EszZj01K9u",
        "outputId": "662b22c4-62ec-42d0-c176-2e359a67f6b8"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdCiLB3shpRk"
      },
      "source": [
        "Installing Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtkDxb9G0FJK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import os\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, Concatenate\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sacrebleu import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uBGj8by-1euR",
        "outputId": "6ce5aa05-4aa5-49af-9e47-c078eec5d2f2"
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Machine Translation/hin.txt\", sep='\\t', header=None, names=[\"english_sentence\",\"hindi_sentence\",\"path\"])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>वाह!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  english_sentence  ...                                               path\n",
              "0             Wow!  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
              "1            Help!  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "2            Jump.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "3            Jump.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "4            Jump.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6bF4yQ61_lZ",
        "outputId": "5ee2bc00-6fb0-4534-b662-9ff48b38806e"
      },
      "source": [
        "df=df.drop(columns=['path'])\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2774, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtEo2riK2OvR",
        "outputId": "15158806-af8e-4412-ea22-8b229e4b49a9"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "english_sentence    0\n",
              "hindi_sentence      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znO8s9Y5hsjn"
      },
      "source": [
        "***Text Pre-Processing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UTTHsiw2S5J"
      },
      "source": [
        "df['english_sentence']=df['english_sentence'].apply(lambda x: x.lower())\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.lower())\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: x.strip())\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: x.strip())\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in string.punctuation))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in string.punctuation))\n",
        "df['english_sentence']=df['english_sentence'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "df['hindi_sentence']=df['hindi_sentence'].apply(lambda x:  re.sub(r'\\d+','',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM7XTUNn2VAw"
      },
      "source": [
        "start = '<s> '\n",
        "end = ' </s>'\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : start + x + end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "nru2403jzo4B",
        "outputId": "4b0b3fda-d22a-4189-88a7-e3bd98e1f77b"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>&lt;s&gt; वाह &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>help</td>\n",
              "      <td>&lt;s&gt; बचाओ &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jump</td>\n",
              "      <td>&lt;s&gt; उछलो &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jump</td>\n",
              "      <td>&lt;s&gt; कूदो &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jump</td>\n",
              "      <td>&lt;s&gt; छलांग &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hello</td>\n",
              "      <td>&lt;s&gt; नमस्ते। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hello</td>\n",
              "      <td>&lt;s&gt; नमस्कार। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cheers</td>\n",
              "      <td>&lt;s&gt; वाहवाह &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cheers</td>\n",
              "      <td>&lt;s&gt; चियर्स &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>got it</td>\n",
              "      <td>&lt;s&gt; समझे कि नहीं &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>im ok</td>\n",
              "      <td>&lt;s&gt; मैं ठीक हूँ। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>awesome</td>\n",
              "      <td>&lt;s&gt; बहुत बढ़िया &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>come in</td>\n",
              "      <td>&lt;s&gt; अंदर आ जाओ। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>get out</td>\n",
              "      <td>&lt;s&gt; बाहर निकल जाओ &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>go away</td>\n",
              "      <td>&lt;s&gt; चले जाओ &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>goodbye</td>\n",
              "      <td>&lt;s&gt; ख़ुदा हाफ़िज़। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>perfect</td>\n",
              "      <td>&lt;s&gt; उत्तम &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>perfect</td>\n",
              "      <td>&lt;s&gt; सही &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>welcome</td>\n",
              "      <td>&lt;s&gt; आपका स्वागत है। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>welcome</td>\n",
              "      <td>&lt;s&gt; स्वागतम्। &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   english_sentence            hindi_sentence\n",
              "0               wow              <s> वाह </s>\n",
              "1              help             <s> बचाओ </s>\n",
              "2              jump             <s> उछलो </s>\n",
              "3              jump             <s> कूदो </s>\n",
              "4              jump            <s> छलांग </s>\n",
              "5             hello          <s> नमस्ते। </s>\n",
              "6             hello         <s> नमस्कार। </s>\n",
              "7            cheers           <s> वाहवाह </s>\n",
              "8            cheers           <s> चियर्स </s>\n",
              "9            got it     <s> समझे कि नहीं </s>\n",
              "10            im ok     <s> मैं ठीक हूँ। </s>\n",
              "11          awesome      <s> बहुत बढ़िया </s>\n",
              "12          come in      <s> अंदर आ जाओ। </s>\n",
              "13          get out    <s> बाहर निकल जाओ </s>\n",
              "14          go away          <s> चले जाओ </s>\n",
              "15          goodbye   <s> ख़ुदा हाफ़िज़। </s>\n",
              "16          perfect            <s> उत्तम </s>\n",
              "17          perfect              <s> सही </s>\n",
              "18          welcome  <s> आपका स्वागत है। </s>\n",
              "19          welcome        <s> स्वागतम्। </s>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oljcpyath25a"
      },
      "source": [
        "Tokenizing the words present in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bya8vsTN37Sv"
      },
      "source": [
        "english_vocab = {}\n",
        "for i in df.english_sentence:\n",
        "  for word in i.split():\n",
        "    if word not in english_vocab:\n",
        "      english_vocab[word] = 1\n",
        "    else:\n",
        "      english_vocab[word]+=1\n",
        "\n",
        "hindi_vocab={}\n",
        "for j in df.hindi_sentence:\n",
        "  for a in j.split():\n",
        "    if a not in hindi_vocab:\n",
        "      hindi_vocab[a] = 1\n",
        "    else:\n",
        "      hindi_vocab[a]+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWITFrAq4sYw",
        "outputId": "a1d4b98b-ff86-4f23-8106-c79b504458f2"
      },
      "source": [
        "num_encoder_tokens=len(english_vocab.keys())\n",
        "num_decoder_token=len(hindi_vocab.keys())\n",
        "length = []\n",
        "for i in df.english_sentence:\n",
        "  length.append(len(i.split(' ')))\n",
        "max_input_length = max(length)\n",
        "print('max_input_length: ', max_input_length)\n",
        "length = []\n",
        "for i in df.hindi_sentence:\n",
        "  length.append(len(i.split(' ')))\n",
        "max_output_length = max(length)\n",
        "print('max_output_length: ', max_output_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_input_length:  22\n",
            "max_output_length:  27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6zRYEt79Ymy"
      },
      "source": [
        "input_words = sorted(list(english_vocab.keys()))\n",
        "target_words = sorted(list(hindi_vocab.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxAcdOUY9fsT"
      },
      "source": [
        "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkMalu_h8np"
      },
      "source": [
        "Converting the word into array of vectors **(Mapping)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfC5rEr9_BsZ"
      },
      "source": [
        "encoder_input_data = np.zeros((len(df.english_sentence), max_input_length), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(df.hindi_sentence), max_output_length), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(df.hindi_sentence), max_output_length, num_decoder_token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKI_4ZUQ_LIY"
      },
      "source": [
        "for i,(input_text, output_text) in enumerate(zip(df.english_sentence, df.hindi_sentence)):\n",
        "  for t, word in enumerate(input_text.split()):\n",
        "    encoder_input_data[i,t] = input_token_index[word]\n",
        "  for t,word in enumerate(output_text.split()):\n",
        "    decoder_input_data[i,t] = target_token_index[word]\n",
        "    if t > 0:\n",
        "      decoder_target_data[i,t-1,target_token_index[word]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4naxw0_H_-rV"
      },
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeC5uBikiGGO"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4n-BumRAC9H"
      },
      "source": [
        "encoder_inputs = Input(shape=(max_input_length,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(units=units//2,return_state=True, return_sequences=True,recurrent_initializer='glorot_uniform'))\n",
        "encoder_outputs, forward_state_h, forward_state_c, backward_state_h, backward_state_c = encoder_lstm(enc_emb)\n",
        "final_state_h = Concatenate()([forward_state_h,backward_state_h])\n",
        "final_state_c = Concatenate()([forward_state_c,backward_state_c])\n",
        "encoder_states = [final_state_h, final_state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk0QFBjuiIfY"
      },
      "source": [
        "**Attention Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5SnhR8HBkmA"
      },
      "source": [
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        " \n",
        " \n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        " \n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        " \n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        " \n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        " \n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        " \n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        " \n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        " \n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        " \n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        " \n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        " \n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        " \n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        " \n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        " \n",
        "            return e_i, [e_i]\n",
        " \n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        " \n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        " \n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        " \n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        " \n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        " \n",
        "        return c_outputs, e_outputs\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIBVN1R3iMNZ"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImgXkVXcBQ4W"
      },
      "source": [
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_token, embedding_dim)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True,recurrent_initializer='glorot_uniform')\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "attn_layer = AttentionLayer()\n",
        "attention_result,attention_weights = attn_layer([encoder_outputs,decoder_outputs])\n",
        "\n",
        "decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_outputs,attention_result])\n",
        "\n",
        "decoder_dense = Dense(num_decoder_token, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNep-k8NJdSq",
        "outputId": "0ce6716c-b8a6-47b7-918f-0be92525fb3e"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 22)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 22, 256)      599808      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 22, 1024), ( 3149824     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    760064      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1024)         0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1024)         0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 1024), 5246976     embedding_1[0][0]                \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 1024), 2098176     bidirectional[0][0]              \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 2048)   0           lstm_1[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2969)   6083481     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 17,938,329\n",
            "Trainable params: 17,938,329\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gfQMi7eiOhL"
      },
      "source": [
        "Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mM-teg4JhKh",
        "outputId": "5e0b0346-75de-46e9-8b21-08b6809b98df"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=100, epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "28/28 [==============================] - 346s 12s/step - loss: 1.9163 - accuracy: 0.5593\n",
            "Epoch 2/25\n",
            "28/28 [==============================] - 338s 12s/step - loss: 1.8182 - accuracy: 0.5725\n",
            "Epoch 3/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.8022 - accuracy: 0.4582\n",
            "Epoch 4/25\n",
            "28/28 [==============================] - 340s 12s/step - loss: 1.7925 - accuracy: 0.4383\n",
            "Epoch 5/25\n",
            "28/28 [==============================] - 342s 12s/step - loss: 1.7810 - accuracy: 0.4895\n",
            "Epoch 6/25\n",
            "28/28 [==============================] - 343s 12s/step - loss: 1.7633 - accuracy: 0.5676\n",
            "Epoch 7/25\n",
            "28/28 [==============================] - 343s 12s/step - loss: 1.7557 - accuracy: 0.5646\n",
            "Epoch 8/25\n",
            "28/28 [==============================] - 343s 12s/step - loss: 1.7408 - accuracy: 0.5163\n",
            "Epoch 9/25\n",
            "28/28 [==============================] - 346s 12s/step - loss: 1.7289 - accuracy: 0.6205\n",
            "Epoch 10/25\n",
            "28/28 [==============================] - 348s 12s/step - loss: 1.7202 - accuracy: 0.5922\n",
            "Epoch 11/25\n",
            "28/28 [==============================] - 343s 12s/step - loss: 1.6993 - accuracy: 0.5470\n",
            "Epoch 12/25\n",
            "28/28 [==============================] - 343s 12s/step - loss: 1.6958 - accuracy: 0.5747\n",
            "Epoch 13/25\n",
            "28/28 [==============================] - 344s 12s/step - loss: 1.6759 - accuracy: 0.5988\n",
            "Epoch 14/25\n",
            "28/28 [==============================] - 341s 12s/step - loss: 1.6682 - accuracy: 0.5733\n",
            "Epoch 15/25\n",
            "28/28 [==============================] - 340s 12s/step - loss: 1.6518 - accuracy: 0.5742\n",
            "Epoch 16/25\n",
            "28/28 [==============================] - 341s 12s/step - loss: 1.6369 - accuracy: 0.6048\n",
            "Epoch 17/25\n",
            "28/28 [==============================] - 340s 12s/step - loss: 1.6211 - accuracy: 0.5718\n",
            "Epoch 18/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.6054 - accuracy: 0.5777\n",
            "Epoch 19/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.5932 - accuracy: 0.6577\n",
            "Epoch 20/25\n",
            "28/28 [==============================] - 342s 12s/step - loss: 1.5855 - accuracy: 0.6061\n",
            "Epoch 21/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.5638 - accuracy: 0.5584\n",
            "Epoch 22/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.5528 - accuracy: 0.5565\n",
            "Epoch 23/25\n",
            "28/28 [==============================] - 341s 12s/step - loss: 1.5353 - accuracy: 0.5627\n",
            "Epoch 24/25\n",
            "28/28 [==============================] - 339s 12s/step - loss: 1.5264 - accuracy: 0.4839\n",
            "Epoch 25/25\n",
            "28/28 [==============================] - 341s 12s/step - loss: 1.5094 - accuracy: 0.4593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd6ae3418d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKHPqKecP7QG",
        "outputId": "3a6d8543-49b6-4648-edec-cbdb31ec3544"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 22)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 22, 256)      599808      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 22, 1024), ( 3149824     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1024)         0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1024)         0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "==================================================================================================\n",
            "Total params: 3,749,632\n",
            "Trainable params: 3,749,632\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qDNK_dbRmYb",
        "outputId": "8003be21-d2b9-41f3-ff02-58db0387d8e9"
      },
      "source": [
        "decoder_state_input_h = Input(shape=(units,))\n",
        "decoder_state_input_c = Input(shape=(units,))\n",
        "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
        "decoder_hidden_state_input = Input(shape=(num_encoder_tokens,units))\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "\n",
        "attention_result_inf, attention_weights_inf = attn_layer([decoder_hidden_state_input,decoder_outputs2])\n",
        "decoder_concatenate_input_inf = Concatenate(axis=-1,name='concat_layer')([decoder_outputs2,attention_result_inf])\n",
        "\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_concatenate_input_inf)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] +[decoder_hidden_state_input,decoder_state_input_h,decoder_state_input_c],\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    760064      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 1024), 5246976     embedding_1[1][0]                \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 2343, 1024)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer multiple             2098176     input_5[0][0]                    \n",
            "                                                                 lstm_1[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 2048)   0           lstm_1[1][0]                     \n",
            "                                                                 attention_layer[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2969)   6083481     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 14,188,697\n",
            "Trainable params: 14,188,697\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}